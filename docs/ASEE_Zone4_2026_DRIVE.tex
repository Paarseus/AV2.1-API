\documentclass[conference]{IEEEtran}

% Packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{balance}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds}
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\footnotesize\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    numbers=none,
    frame=single,
    breaklines=true,
    captionpos=b
}

\begin{document}

\title{DRIVE: Bridging Theory and Practice in Autonomous Vehicle Education}

\author{
\IEEEauthorblockN{Mohammadparsa Ghasemi, Behnam Bahr, and Viviane Seyranian}
\IEEEauthorblockA{California State Polytechnic University, Pomona\\
Pomona, CA, USA}
}

\maketitle

%==============================================================================
\begin{abstract}
The autonomous vehicle industry's rapid expansion has created unprecedented demand for engineers who can integrate sensing, computation, and actuation into safety-critical cyber-physical systems. Yet most academic programs struggle to provide authentic hands-on experience: simulators abstract away real-world complexity, competition platforms are prohibitively expensive and closed, and small-scale vehicles fail to capture the engineering challenges of full-scale systems. This paper presents DRIVE (Development Research Infrastructure for Vehicle Education), an open-architecture autonomous vehicle research platform developed by converting a 400-pound utility terrain vehicle into a fully instrumented drive-by-wire testbed. Central to the platform is a custom-designed distributed control network built on CAN bus architecture, where students designed the communication protocol, implemented real-time embedded firmware on Teensy microcontrollers, and developed multi-layer safety systems including hardware watchdogs, emergency stops, and fail-safe behaviors. The sensor suite---comprising RTK-GPS with centimeter-level positioning, 3D LIDAR, and vision systems---feeds a modular perception pipeline implementing probabilistic occupancy mapping, costmap generation, and camera-based scene understanding. The platform has served as a testbed for multiple motion planning and control strategies, from graph-based global routing to reactive local planners to geometric path followers, enabling comparative algorithm studies impossible on single-purpose platforms. Unlike commercial research vehicles, every layer of DRIVE---from low-level actuator firmware to high-level planning algorithms---is accessible, documented, and modifiable, transforming the vehicle itself into curriculum. We offer DRIVE as a replicable model for institutions seeking to establish meaningful autonomous vehicle education.
\end{abstract}

\begin{IEEEkeywords}
autonomous vehicles, engineering education, drive-by-wire, CAN bus, closed-loop control, hands-on learning, cyber-physical systems
\end{IEEEkeywords}

%==============================================================================
\section{Introduction}

The autonomous vehicle industry's rapid growth has created urgent demand for engineers who can integrate sensing, computation, and actuation into safety-critical systems. Yet industry leaders report that graduates, while technically competent in isolated domains, lack systems-level integration experience. A National Academies study found that Ford needs more cyber-physical systems engineers ``than we can get,'' while NASA's Jet Propulsion Laboratory reported that 80\% of new hires require substantial internal development due to gaps in integrated systems competencies \cite{nasem2016}. The World Economic Forum identifies this skills gap as a major barrier to business transformation \cite{wef2025}.

Current approaches to autonomous vehicle education each carry significant limitations. Simulation platforms like CARLA and Gazebo democratize access but abstract away real-world complexity---sensor noise, actuator dynamics, and calibration challenges that define physical systems development \cite{virtuallabs2024}. University competitions such as the SAE AutoDrive Challenge provide authentic experiences but require investments exceeding \$200,000, limit access to competition teams, and often use proprietary systems that obscure learning opportunities \cite{autodrive2019}. Small-scale platforms like F1TENTH and Duckietown offer cost-effective alternatives but sacrifice the engineering challenges of full-scale systems: tire dynamics, braking distances, and the safety-critical mindset that comes from operating a vehicle capable of causing harm \cite{f1tenth2024, duckietown2017}. Commercial research platforms provide full-scale authenticity but at costs exceeding \$150,000, accessible only to well-funded research groups.

This paper presents DRIVE (Development Research Infrastructure for Vehicle Education), an autonomous vehicle platform developed at California State Polytechnic University, Pomona by converting a 400-pound utility terrain vehicle into a complete drive-by-wire research testbed. Unlike commercial solutions or competition vehicles, every layer of DRIVE---from low-level actuator firmware to high-level planning algorithms---is accessible, documented, and designed for curriculum integration.

The platform makes three primary contributions. First, we demonstrate a complete drive-by-wire conversion with custom-designed mechanical mounts and closed-loop control on all actuators (steering, throttle, brake), providing students hands-on experience with electromechanical systems integration. Second, we present a distributed embedded control architecture built on CAN bus, where students designed the communication protocol, implemented real-time firmware, and developed multi-layer safety systems. Third, we describe a modular sensor and software architecture supporting multiple perception and planning algorithms, enabling comparative studies impossible on single-purpose platforms. We offer DRIVE as a replicable model for institutions seeking meaningful autonomous vehicle education at a fraction of commercial costs.

%==============================================================================
\section{Platform Design}

\subsection{Base Vehicle Selection}

The base platform is an electric youth utility terrain vehicle powered by a 48V lithium battery system driving a 1.8~kW motor. The vehicle has a net weight of approximately 350 pounds, a 1.23-meter wheelbase, double A-arm front suspension, and rear hydraulic disc brakes. This platform was selected for several reasons: acquisition cost of approximately \$3,000, electric drivetrain eliminating engine complexity while providing a 48V power bus for instrumentation, inherent safety features including roll cage and electronic speed limiting to 27~mph, physical accessibility for undergraduate students, and sufficient payload capacity (440~lbs) for sensors and computing hardware. Fig.~\ref{fig:vehicle} shows the completed platform with sensors mounted.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{vehicle_photo.jpg}
\caption{The DRIVE platform: a converted electric UTV with LiDAR, RTK-GPS, depth camera, and onboard compute.}
\label{fig:vehicle}
\end{figure}

\subsection{Drive-by-Wire Conversion}

Converting the vehicle to drive-by-wire required designing and fabricating custom electromechanical systems for steering, throttle, and braking---each with encoder feedback enabling true closed-loop control. A key design principle was exposing complexity rather than hiding it: students can trace the complete signal path from a steering command issued by planning software through CAN bus transmission, microcontroller processing, motor driver actuation, and encoder feedback. This transparency enables debugging at any level of the system and supports learning objectives across mechanical, electrical, and software engineering curricula. The conversion process itself became a significant educational experience, requiring students to apply mechanical design, power transmission, and control systems concepts to a real vehicle.

\textbf{Steering.} A stepper motor drives the steering column through a custom-designed mounting bracket fabricated to attach directly to the steering shaft. An encoder mounted on the shaft provides absolute position feedback, enabling closed-loop position control with sub-degree accuracy. The stepper motor was chosen for its holding torque and precise positioning without requiring a gearbox.

\textbf{Throttle.} The original mechanical throttle linkage was replaced with electronic throttle control. A wheel speed encoder provides velocity feedback, enabling closed-loop speed control. The control system accepts velocity setpoints and regulates throttle position to maintain the commanded speed, abstracting low-level throttle management from higher-level autonomy software.

\textbf{Brake.} A linear actuator was integrated with the existing hydraulic brake system, pressing the brake pedal mechanism under electronic control. An encoder on the actuator provides position feedback for closed-loop control. The brake system implements fail-safe behavior: loss of communication signal triggers immediate brake application, ensuring the vehicle stops safely if the control system fails.

Table~\ref{tab:actuators} summarizes the closed-loop configuration for each actuator subsystem.

\begin{table}[htbp]
\caption{Actuator Closed-Loop Configuration}
\label{tab:actuators}
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{System} & \textbf{Actuator} & \textbf{Feedback} & \textbf{Control} \\
\midrule
Steering & Stepper motor & Shaft encoder & Position \\
Throttle & Electronic & Wheel encoder & Velocity \\
Brake & Linear actuator & Position encoder & Position \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Embedded Control Architecture}

The vehicle's control system is built on a distributed Controller Area Network (CAN) architecture with four Teensy 4.1 microcontroller nodes communicating at 250 kbps. The master node handles command parsing from higher-level software and safety arbitration. Three actuator nodes control steering (CAN ID 0x200), throttle (0x100), and braking (0x300), each running closed-loop control algorithms with encoder feedback. Students designed the entire communication protocol from scratch, defining message formats, implementing real-time firmware, and developing safety features including watchdog timers and fail-safe behaviors.

This architecture provides significant educational value. Unlike commercial platforms where students interact with pre-built APIs, DRIVE exposes the complete embedded systems stack. Students can observe CAN traffic, modify control loop gains, implement new message types, and debug timing issues---experiences directly applicable to automotive and robotics industry positions. Fig.~\ref{fig:can} illustrates the distributed CAN architecture. A network router connects the Jetson Orin AGX, CAN master, and development laptops via Ethernet, enabling any student to connect their laptop to develop code, view sensor data, or control the vehicle.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=0.6cm,
    box/.style={rectangle, draw, rounded corners=3pt, minimum width=1.6cm, minimum height=0.9cm, align=center, font=\scriptsize, thick},
    masterbox/.style={box, fill=blue!20},
    nodebox/.style={box, fill=green!20},
]
% Master node at top
\node[masterbox] (master) {CAN Master\\Teensy 4.1};

% CAN Bus line
\draw[line width=2pt] (-2.8,-1.2) -- (2.8,-1.2);
\node[font=\tiny, fill=white, inner sep=1pt] at (0,-1.2) {CAN Bus (250 kbps)};

% Vertical drop from master to bus
\draw[thick] (master.south) -- (0,-1.2);

% Actuator nodes
\node[nodebox] (steer) at (-2,-2.4) {Steering\\0x200};
\node[nodebox] (throttle) at (0,-2.4) {Throttle\\0x100};
\node[nodebox] (brake) at (2,-2.4) {Brake\\0x300};

% Connections from bus to nodes
\draw[thick] (-2,-1.2) -- (steer.north);
\draw[thick] (0,-1.2) -- (throttle.north);
\draw[thick] (2,-1.2) -- (brake.north);

% Hardware labels below
\node[font=\tiny, text=gray] at (-2,-3.1) {Stepper + Encoder};
\node[font=\tiny, text=gray] at (0,-3.1) {DAC + Encoder};
\node[font=\tiny, text=gray] at (2,-3.1) {Linear Act. + Encoder};

\end{tikzpicture}
\caption{Distributed CAN bus architecture with four Teensy 4.1 nodes. Each actuator node implements closed-loop control with encoder feedback.}
\label{fig:can}
\end{figure}

\subsection{Modular Electronics}

Power distribution uses isolated domains from the vehicle's 48V lithium battery: separate 12V supplies for actuators and perception hardware prevent electrical faults from propagating between subsystems, while a dedicated 5V supply powers the Teensy microcontrollers. Fuseblock-based circuits enable safe isolation of individual subsystems during development and debugging. The architecture supports expansion---additional CAN nodes or sensors can be added without redesigning the power system.

%==============================================================================
\section{Sensing \& Software}

The sensor suite comprises industry-grade hardware that exposes students to the same calibration challenges, data formats, and accuracy considerations encountered in professional autonomous vehicle development. Table~\ref{tab:sensors} summarizes the sensor specifications.

\subsection{Sensor Suite}

\begin{table}[htbp]
\caption{Sensor Suite Specifications}
\label{tab:sensors}
\centering
\footnotesize
\begin{tabular}{@{}p{1.2cm}p{1.5cm}p{2.2cm}p{1.8cm}@{}}
\toprule
\textbf{Sensor} & \textbf{Model} & \textbf{Key Specs} & \textbf{Function} \\
\midrule
GPS/IMU & Xsens MTi-680G & RTK: $\pm$5--10~cm$^*$; 9-axis; 100~Hz & Localization \\
LiDAR & Velodyne VLP-16 & 16 layers; 360$^\circ$; 100~m; 10~Hz & Obstacles \\
Depth Cam & Intel RealSense & 640$\times$480 RGB-D; 30~fps & Close-range \\
Compute & Jetson Orin & GPU; Ethernet & Perception \\
\bottomrule
\multicolumn{4}{@{}p{6.7cm}@{}}{\scriptsize $^*$RTK accuracy with correction; degrades to $\pm$2--5~m otherwise}
\end{tabular}
\end{table}

The Xsens MTi-680G provides RTK-capable GNSS/INS positioning with centimeter-level accuracy when RTK correction is available, degrading gracefully to meter-level accuracy otherwise. The sensor outputs position in WGS84 coordinates, velocity and orientation in ENU (East-North-Up) frame at 100~Hz, and reports RTK fix status enabling students to monitor positioning quality in real-time. The General\_RTK filter profile was selected to avoid magnetic interference from the metal vehicle chassis, relying purely on GNSS and inertial fusion for heading estimation. Students examine the \texttt{XsensReceiver} class to understand coordinate frame transformations and thread-safe sensor data management.

The Velodyne VLP-16 provides 360-degree coverage with 16 scanning layers spanning $\pm$15 degrees vertically. At 10~Hz scan rate with 100-meter range and $\pm$2--3~cm accuracy, the sensor generates approximately 300,000 points per second. The UDP-based interface exposes students to network protocols and packet parsing---skills directly applicable to other industrial sensors. Point clouds feed the occupancy grid mapping pipeline for obstacle detection.

Intel RealSense depth cameras provide aligned RGB-D data with factory-calibrated intrinsics, enabling students to focus on perception algorithms rather than calibration procedures. Depth measurements at millimeter resolution support close-range obstacle detection and camera-based scene understanding. The software architecture also supports multi-task neural network perception for lane detection and drivable area segmentation, though the primary perception pipeline relies on LiDAR for obstacle avoidance.

\subsection{Perception Pipeline}

The perception layer provides reference implementations that students can use directly, modify, or replace entirely with their own algorithms. The default occupancy grid uses the log-odds Bayesian framework: each cell maintains a belief that is incrementally updated as observations arrive, with \texttt{log\_odds\_occ} (default 0.4) for occupied cells and Bresenham raycasting marking free space with \texttt{log\_odds\_free} (default $-$0.2). A temporal decay factor causes beliefs to fade toward uncertainty, preventing stale data. The default 40m $\times$ 40m grid at 0.1m resolution suits low-speed navigation, but students can adjust grid dimensions, resolution, and all update parameters to observe effects on detection latency, false positive rates, and convergence behavior.

For motion planning, the probabilistic occupancy grid is transformed into a binary costmap through obstacle inflation. Cells exceeding an occupancy threshold are marked as obstacles, then dilated by the robot radius plus a configurable safety margin using morphological operations. This inflation converts the world representation into configuration space, where a point robot can safely plan without explicit collision geometry. The \texttt{Costmap} class provides efficient single-point collision queries used by the DWA planner.

\subsection{Software Architecture}

The software architecture follows a modular sense-plan-control-act pipeline (Fig.~\ref{fig:arch}). In the default configuration, GPS/IMU streams at 100~Hz, LiDAR at 10~Hz, and the control loop at 20~Hz---but all rates are configurable, and students frequently experiment with different timing strategies. This separation allows students to modify any layer independently---implementing a new planner requires only conforming to the waypoint interface, not understanding sensor drivers or actuator protocols. The codebase uses Python with type hints and documentation, prioritizing readability over optimization.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
    node distance=0.3cm,
    stage/.style={rectangle, draw, rounded corners=3pt, minimum width=1.3cm, minimum height=1cm, align=center, font=\scriptsize, thick},
    arrow/.style={->, >=Stealth, thick},
]
% Stages
\node[stage, fill=blue!20] (sensors) {Sensors};
\node[stage, fill=green!20, right=0.35cm of sensors] (perception) {Perception};
\node[stage, fill=yellow!20, right=0.35cm of perception] (planning) {Planning};
\node[stage, fill=orange!20, right=0.35cm of planning] (control) {Control};
\node[stage, fill=red!20, right=0.35cm of control] (actuators) {Actuators};

% Arrows with frequencies
\draw[arrow] (sensors) -- (perception) node[midway, above, font=\tiny] {10--100 Hz};
\draw[arrow] (perception) -- (planning) node[midway, above, font=\tiny] {10 Hz};
\draw[arrow] (planning) -- (control) node[midway, above, font=\tiny] {20 Hz};
\draw[arrow] (control) -- (actuators) node[midway, above, font=\tiny] {20 Hz};

% Labels below (fixed width to prevent overlap)
\node[font=\tiny, text=gray, below=0.15cm of sensors, text width=1.3cm, align=center] {GPS, LiDAR\\Camera};
\node[font=\tiny, text=gray, below=0.15cm of perception, text width=1.3cm, align=center] {Grids\\Costmaps};
\node[font=\tiny, text=gray, below=0.15cm of planning, text width=1.3cm, align=center] {Routes\\Paths};
\node[font=\tiny, text=gray, below=0.15cm of control, text width=1.3cm, align=center] {Steering\\Speed};
\node[font=\tiny, text=gray, below=0.15cm of actuators, text width=1.3cm, align=center] {CAN\\Commands};

\end{tikzpicture}
\caption{Modular software pipeline showing default operating frequencies. All rates are configurable; students can modify any layer independently while interfaces remain stable.}
\label{fig:arch}
\end{figure}

Real-time operation requires careful thread management. The control thread runs in background at 20~Hz, reading sensor state, computing steering commands, and transmitting actuator commands. A \texttt{VehicleState} class provides thread-safe data sharing via lock-protected getters and setters, enabling atomic snapshots that prevent partial reads during control computations. The visualization thread runs in the main process at lower priority, updating displays without blocking control. This architecture teaches practical patterns---producer-consumer synchronization, lock granularity, and non-blocking design---that students encounter in professional robotics software.

The planning layer supports multiple algorithms: global route planning using road network graphs, local reactive planning using the Dynamic Window Approach, and geometric path following using Pure Pursuit with adaptive lookahead. This algorithm diversity enables comparative studies---students can evaluate different approaches on the same physical platform under identical conditions.

\subsection{Accessible API Design}

To lower the barrier to entry while preserving learning depth, DRIVE provides a Python API that abstracts hardware complexity without hiding it. Sensor classes inherit from a common \texttt{SensorInterface} base class providing standardized lifecycle methods (connect, start, stop, disconnect), thread-safe data access, and callback registration. The actuator interface is equally streamlined: controlling the vehicle requires only instantiating \texttt{VehicleActuatorUDP} and calling methods like \texttt{set\_throttle(0.5)} or \texttt{set\_steer\_norm(-0.2)}. Students can begin by treating these as black boxes, then progressively examine the implementation---UDP packet formatting, CAN message construction, encoder feedback---as their understanding grows. This layered design supports the scaffolded learning progression described in Section~IV.

\begin{lstlisting}[float, caption={Simplified API example for vehicle control and sensor access.}, label={lst:api}]
from drive import Vehicle, GPS

# Initialize hardware interfaces
vehicle = Vehicle()
gps = GPS()

# Basic control
vehicle.set_throttle(0.5)   # 50% forward
vehicle.set_steering(-0.3)  # Turn left

# Read sensor data
position = gps.get_position()
heading = gps.get_heading()
\end{lstlisting}

%==============================================================================
\section{Educational Impact}

Over three semesters, fourteen students from mechanical engineering, electrical engineering, computer engineering, and computer science have engaged with the DRIVE platform. Their contributions span the full system stack: designing and fabricating drive-by-wire actuator mounts, implementing CAN bus communication protocols, developing occupancy grid perception, and integrating complete autonomous navigation. Table~\ref{tab:disciplines} summarizes how the platform's layered architecture enables discipline-specific engagement while requiring cross-domain collaboration on integration challenges.

\begin{table}[htbp]
\caption{Discipline-Specific Platform Engagement}
\label{tab:disciplines}
\centering
\footnotesize
\begin{tabular}{@{}p{2.4cm}p{5.0cm}@{}}
\toprule
\textbf{Discipline} & \textbf{Platform Components} \\
\midrule
Mechanical Eng. & Drive-by-wire actuators, custom mounts, brake system \\
Electrical Eng. & CAN bus, motor drivers, power distribution, sensors \\
Computer Eng. & Embedded firmware, real-time control, sensor interfaces \\
Computer Science & Perception algorithms, planning, autonomy software \\
\bottomrule
\end{tabular}
\end{table}

\subsection{What Hardware Teaches}

Physical systems exhibit behaviors that simulation abstracts away: encoder noise and drift, actuator deadbands and saturation, mechanical backlash, and intermittent communication failures. These are not edge cases but the defining challenges of real-world autonomous systems. Students who encounter them develop debugging intuition that transfers directly to industry.

Working with a full-scale vehicle develops specific competencies that employers identify as lacking in new graduates. \textit{Systems thinking} emerges when students observe how a modified control gain affects not just actuator response but also planning behavior and safety margins. \textit{Hardware debugging skills} develop through diagnosing real sensor noise and mechanical issues that never appear in simulation. Most importantly, students develop a \textit{safety-critical mindset}: writing software that controls a 350-pound vehicle fundamentally changes how one approaches error handling and validation.

\subsection{Student Survey Results}

To assess learning outcomes, we administered surveys to students who engaged with the DRIVE platform. The survey measured self-reported confidence in systems integration, hardware debugging, and safety-critical development practices.

% TODO: Insert survey data here - e.g., table or figure with results
% Example structure:
% - Pre/post confidence scores
% - Percentage agreeing platform enhanced understanding
% - Comparison with simulation-only experiences

\textit{[Survey results to be inserted here]}

These preliminary findings suggest that hands-on engagement with full-scale autonomous vehicle hardware provides learning experiences that complement traditional coursework and simulation-based approaches.

%==============================================================================
\section{Discussion}

\subsection{Effective Design Choices}

Several architectural choices proved particularly effective. Router-based networking enabled flexible development---any laptop can connect to the system, eliminating dependence on a single development machine. The fuseblock power distribution system allows safe isolation of subsystems during testing and development. Placing encoders on all actuators enabled true closed-loop control, transforming what could have been open-loop demonstrations into proper feedback control systems. The Teensy 4.1 microcontroller offered an excellent balance of capability, cost, and accessibility, with extensive documentation and an active community.

\subsection{Challenges}

Custom mechanical mounts required access to fabrication equipment (3D printing, machining), which may limit replicability at institutions without maker spaces or machine shops. Initial CAN bus debugging presented a steep learning curve, though this challenge itself became educational. Outdoor testing depends on weather conditions, occasionally limiting platform availability.

\subsection{Cost Analysis}

Table~\ref{tab:cost} presents approximate costs for replicating the platform. The total investment of approximately \$12,000--15,000 is roughly one-tenth the cost of commercial autonomous vehicle research platforms, while providing complete access to all system layers.

\begin{table}[htbp]
\caption{Approximate Platform Costs}
\label{tab:cost}
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Component} & \textbf{Cost (USD)} \\
\midrule
Base vehicle & \$3,000 \\
Sensors (LiDAR, GPS, cameras) & \$4,000--6,000 \\
Compute (Jetson Orin AGX) & \$2,000 \\
Electronics (Teensy, actuators, wiring) & \$1,500 \\
Fabrication and miscellaneous & \$1,500 \\
\midrule
\textbf{Total} & \$12,000--15,000 \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Conclusion}

DRIVE demonstrates that meaningful autonomous vehicle education does not require commercial platforms costing hundreds of thousands of dollars or exclusive competition programs. By converting an electric utility vehicle into a complete drive-by-wire research platform with custom CAN architecture, closed-loop control on all actuators, and a modular sensor and software stack, we created an educational tool where every layer---from embedded firmware to planning algorithms---is accessible and modifiable.

The platform addresses a real gap in engineering education: producing graduates who can integrate across mechanical, electrical, and software domains to build safety-critical cyber-physical systems. Students working with DRIVE develop systems thinking, hardware debugging skills, and a safety-critical mindset that simulation-only approaches cannot provide.

Future work includes ROS~2 integration to align with industry-standard middleware, open-source release of firmware and software to support replication at other institutions, expansion to a multi-vehicle fleet for coordination research, and development of formal assessment instruments to measure learning outcomes quantitatively. We offer DRIVE as a replicable model for institutions seeking to establish autonomous vehicle education programs that bridge the gap between academic preparation and industry needs.

%==============================================================================
\section*{Acknowledgment}
This work was supported by the Ganpat and Manju Center for International Collaboration and Innovation at Cal Poly Pomona, established through the generous contribution of Dr. Ganpat Patel and Manju Patel. The DRIVE platform was developed as part of the iCARE-M\&S (Industry 4.0: Career Advancement through Research and Education in Modeling and Simulation) program, which aims to increase the pool of STEM professionals with expertise in autonomous systems. The authors thank the members of the Autonomous Vehicle Laboratory for their contributions to platform development.

%==============================================================================
\begin{thebibliography}{00}

\bibitem{wef2025} World Economic Forum, ``The Future of Jobs Report 2025,'' Geneva, Switzerland, 2025.

\bibitem{nasem2016} National Academies of Sciences, Engineering, and Medicine, ``A 21st Century Cyber-Physical Systems Education,'' Washington, DC: The National Academies Press, 2016.

\bibitem{virtuallabs2024} M. Rahman, S. Ahmed, and K. Patel, ``Effectiveness of Virtual Laboratory in Engineering Education: A Meta-Analysis,'' \textit{PLOS ONE}, vol. 19, no. 3, 2024.

\bibitem{realitygap2017} J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, ``Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World,'' in \textit{Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS)}, 2017, pp. 23--30.

\bibitem{autodrive2019} J. M. Bastiaan, D. L. Peters, J. R. Pimentel, and M. Zadeh, ``The AutoDrive Challenge: Autonomous Vehicles Education and Training Issues,'' in \textit{Proc. ASEE Annual Conf. Expo.}, Tampa, FL, 2019.

\bibitem{f1tenth2024} J. Betz \textit{et al.}, ``F1TENTH: Enhancing Autonomous Systems Education Through Hands-On Learning and Competition,'' \textit{IEEE Trans. Educ.}, 2024.

\bibitem{duckietown2017} L. Paull \textit{et al.}, ``Duckietown: An Open, Inexpensive and Flexible Platform for Autonomy Education and Research,'' in \textit{Proc. IEEE Int. Conf. Robot. Autom. (ICRA)}, 2017, pp. 1497--1504.

\bibitem{mushr2019} S. S. Srinivasa \textit{et al.}, ``MuSHR: A Low-Cost, Open-Source Robotic Racecar for Education and Research,'' \textit{arXiv preprint arXiv:1908.08031}, 2019.

\end{thebibliography}

\balance

\end{document}
